{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e456bd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully imported RAGService from src.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Add the project root to system path so we can import from src\n",
    "# We use os.path.abspath to ensure we get the correct absolute path\n",
    "project_root = os.path.abspath('..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "try:\n",
    "    from src.rag_pipeline import RAGService\n",
    "    print(\"✅ Successfully imported RAGService from src.\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing src: {e}\")\n",
    "    print(\"Make sure you are running this notebook from the 'notebooks' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ced74cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vector Store from: d:\\10academy\\phase5\\rag-complaint-chatbot\\vector_store\n",
      "Initializing RAG Service from d:\\10academy\\phase5\\rag-complaint-chatbot\\vector_store...\n",
      "Loading LLM (google/flan-t5-large)...\n",
      "Error loading LLM: The paging file is too small for this operation to complete. (os error 1455)\n",
      "❌ Failed to load RAG Service: The paging file is too small for this operation to complete. (os error 1455)\n"
     ]
    }
   ],
   "source": [
    "# Define path to vector store\n",
    "VECTOR_STORE_PATH = os.path.join(project_root, \"vector_store\")\n",
    "\n",
    "print(f\"Loading Vector Store from: {VECTOR_STORE_PATH}\")\n",
    "\n",
    "if not os.path.exists(VECTOR_STORE_PATH):\n",
    "    print(\"❌ Error: Vector store directory not found.\")\n",
    "    print(\"Please run '02_chunking_embedding.ipynb' to generate the database first.\")\n",
    "else:\n",
    "    try:\n",
    "        # Initialize the service\n",
    "        # This will load the embedding model and the LLM\n",
    "        rag_service = RAGService(vector_store_path=VECTOR_STORE_PATH)\n",
    "        print(\"✅ RAG Service loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load RAG Service: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Questions relevant to CrediTrust's products\n",
    "test_questions = [\n",
    "    \"What are the main issues customers have with Money Transfers?\",\n",
    "    \"Why are people complaining about Credit Card late fees?\",\n",
    "    \"Are there issues with accessing Savings Accounts online?\",\n",
    "    \"What specific companies are mentioned in student loan complaints?\", # Test specific entity retrieval\n",
    "    \"Tell me about a positive experience.\" # Test handling of missing info (complaints usually aren't positive)\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(test_questions)} questions for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed61244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if service loaded correctly\n",
    "if 'rag_service' in locals():\n",
    "    results = []\n",
    "    \n",
    "    print(\"Starting evaluation loop...\")\n",
    "    \n",
    "    for i, q in enumerate(test_questions):\n",
    "        print(f\"\\n[{i+1}/{len(test_questions)}] Question: {q}\")\n",
    "        \n",
    "        try:\n",
    "            # Get answer and sources\n",
    "            answer, docs = rag_service.answer_question(q)\n",
    "            \n",
    "            # Extract info from the first source (if available) for validation\n",
    "            source_id = docs[0].metadata.get('complaint_id', 'N/A') if docs else \"None\"\n",
    "            source_preview = docs[0].page_content[:100].replace('\\n', ' ') + \"...\" if docs else \"None\"\n",
    "            \n",
    "            # Store result\n",
    "            results.append({\n",
    "                \"Question\": q,\n",
    "                \"Generated Answer\": answer.strip(),\n",
    "                \"Source 1 ID\": source_id,\n",
    "                \"Source 1 Preview\": source_preview\n",
    "            })\n",
    "            print(f\"   -> Answer generated ({len(answer)} chars).\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   -> Error processing question: {e}\")\n",
    "            results.append({\n",
    "                \"Question\": q,\n",
    "                \"Generated Answer\": f\"ERROR: {str(e)}\",\n",
    "                \"Source 1 ID\": \"ERROR\",\n",
    "                \"Source 1 Preview\": \"ERROR\"\n",
    "            })\n",
    "\n",
    "    print(\"\\nEvaluation complete.\")\n",
    "else:\n",
    "    print(\"Skipping evaluation: RAG Service not loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0acbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results' in locals() and results:\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Configure pandas to show full text\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    \n",
    "    print(\"Evaluation Results Summary:\")\n",
    "    display(df_results)\n",
    "else:\n",
    "    print(\"No results to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0703b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_results' in locals():\n",
    "    # Define output path\n",
    "    output_path = os.path.join(project_root, \"data\", \"processed\", \"rag_evaluation_results.csv\")\n",
    "    \n",
    "    # Ensure directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Save\n",
    "    df_results.to_csv(output_path, index=False)\n",
    "    print(f\"✅ Evaluation results saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
